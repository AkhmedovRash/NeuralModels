import numpy as np
from tensorflow.keras.models import Model, load_model
import re
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import Dense, Embedding, Input, concatenate, Activation, MaxPooling1D, Conv1D, BatchNormalization, Dropout, Conv2DTranspose, Conv1DTranspose, Lambda
from tensorflow.keras import backend as K
from tensorflow.keras.optimizers import Adam, Adadelta
from tensorflow.keras import utils
import tensorflow
from google.colab import files
import matplotlib.pyplot as plt
from gensim.models import word2vec
import os
import pandas as pd
import time
import nltk
from nltk.stem import WordNetLemmatizer
import pymorphy2

nltk.download('wordnet')

def readText(fileName):
    f = open(fileName, 'r')
    text = f.read()
    delSymbols = ['\n', "\t", "\ufeff", ".", "_", "-", ",", "!", "?", "–", "(", ")", "«", "»", "№", ";", '•', '%']
    for dS in delSymbols:
        text = text.replace(dS, " ")
        text = re.sub("[.]", " ", text)
    text = re.sub(":", " ", text)
    text = re.sub("<", " <", text)
    text = re.sub(">", "> ", text)
    text = ' '.join(text.split())
    text = text.lower()
    return text

def text2Words(text):
    morph = pymorphy2.MorphAnalyzer()
    words = text.split(' ')
    docs = [morph.parse(word)[0].normal_form for word in words]
    return docs

directory = '/content/drive/MyDrive/Договора432/'
curTime = time.time()
agreements = []
for filename in os.listdir(directory):
    txt = readText(directory + filename)
    if txt != '':
        agreements.append(txt)
print('Загрузка файла заняла: ', round(time.time() - curTime, 2), 'с.')

n = 7
docs_full = [text2Words(agreements[i]) for i in range(len(agreements))]
docs = docs_full[0:-10]
docsToTest = docs_full[-10:]
tokenizer = Tokenizer(lower=True, filters='', char_level=False)
tokenizer.fit_on_texts(docs_full)
clean_voc = {item[0]: item[1] for item in tokenizer.word_index.items()}
tok_agreem = tokenizer.texts_to_sequences(docs)

def getXYSamples(tok_agreem, tags_index):
    tags01 = []
    indices = []
    for agreement in tok_agreem:
        tag_place = [0, 0, 0, 0, 0, 0]
        for ex in agreement:
            if ex in tags_index:
                place = np.argwhere(tags_index == ex)
                if len(place) != 0:
                    if place[0][0] < 6:
                        tag_place[place[0][0]] = 1
                    else:
                        tag_place[place[0][0] - 6] = 0
            else:
                tags01.append(tag_place.copy())
                indices.append(ex)
    return indices, tags01

def reverseIndex(clean_voc, x):
    reverse_word_map = dict(map(reversed, clean_voc.items()))
    docs = [reverse_word_map.get(letter) for letter in x]
    return docs

tags_index = ['<s' + str(i) + '>' for i in range(1, 7)]
closetags = ['</s' + str(i) + '>' for i in range(1, 7)]
tags_index.extend(closetags)
tags_index = np.array([clean_voc[i] for i in tags_index])

xData, yData = getXYSamples(tok_agreem, tags_index)
decoded_text = reverseIndex(clean_voc, xData)

def getSetFromIndices(wordIndices, xLen, step):
    xBatch = []
    index = 0
    wordsLen = len(wordIndices)
    while (index + xLen <= wordsLen):
        xBatch.append(wordIndices[index:index+xLen])
        index += step
    return xBatch

xLen = 256
step = 30
embeddingSize = 300
xTrain = getSetFromIndices(decoded_text, xLen, step)
yTrain = getSetFromIndices(yData, xLen, step)
tok_agreemTest = tokenizer.texts_to_sequences(docsToTest)
xDataTest, yDataTest = getXYSamples(tok_agreemTest, tags_index)
decoded_text = reverseIndex(clean_voc, xDataTest)
xTest = getSetFromIndices(decoded_text, xLen, step)
yTest = getSetFromIndices(yDataTest, xLen, step)

def dice_coef(y_true, y_pred):
    return (2. * K.sum(y_true * y_pred) + 1.) / (K.sum(y_true) + K.sum(y_pred) + 1.)

def create_Conv1d(xLen, embeddingSize):
    text_input_layer = Input((xLen, embeddingSize))
    text_layer = Conv1D(16, 3, padding='same', activation='relu')(text_input_layer)
    text_layer = Conv1D(16, 3, padding='same', activation='relu')(text_layer)
    text_layer = Conv1D(16, 3, padding='same', activation='relu')(text_layer)
    text_layer = Conv1D(GENSIMtrainY.shape[-1], 3, padding='same', activation='sigmoid')(text_layer)
    model = Model(text_input_layer, text_layer)
    model.compile(optimizer=Adadelta(), loss='categorical_crossentropy', metrics=[dice_coef])
    return model

def create_PSPNet(conv_size=64, num_classes=6, input_shape=(30, 300)):
    img_input = Input(input_shape)
    x = Conv1D(conv_size, 3, padding='same')(img_input)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Conv1D(conv_size, 3, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x_mp_2 = MaxPooling1D(2)(x)
    x_mp_4 = MaxPooling1D(4)(x)
    x_mp_8 = MaxPooling1D(8)(x)
    x_mp_16 = MaxPooling1D(16)(x)
    x_mp_32 = MaxPooling1D(32)(x)
    x_mp_2 = Conv1D(conv_size, 3, padding='same', activation='relu')(x_mp_2)
    x_mp_4 = Conv1D(conv_size, 3, padding='same', activation='relu')(x_mp_4)
    x_mp_8 = Conv1D(conv_size, 3, padding='same', activation='relu')(x_mp_8)
    x_mp_16 = Conv1D(conv_size, 3, padding='same', activation='relu')(x_mp_16)
    x_mp_32 = Conv1D(conv_size, 3, padding='same', activation='relu')(x_mp_32)
    x_mp_2 = Conv1DTranspose(conv_size, 2, strides=2)(x_mp_2)
    x_mp_4 = Conv1DTranspose(conv_size, 4, strides=4)(x_mp_4)
    x_mp_8 = Conv1DTranspose(conv_size, 8, strides=8)(x_mp_8)
    x_mp_16 = Conv1DTranspose(conv_size, 16, strides=16)(x_mp_16)
    x_mp_32 = Conv1DTranspose(conv_size, 32, strides=32)(x_mp_32)
    fin = concatenate([img_input, x_mp_2, x_mp_4, x_mp_8, x_mp_16, x_mp_32])
    fin = Conv1D(conv_size, 3, padding='same')(fin)
    fin = BatchNormalization()(fin)
    fin = Activation('relu')(fin)
    fin = Conv1D(num_classes, 3, activation='sigmoid', padding='same')(fin)
    model = Model(img_input, fin)
    model.compile(optimizer=Adadelta(lr=0.001), loss='categorical_crossentropy', metrics=[dice_coef])
    return model

def create_unet(k=1, num_classes=6, input_shape=(30, 300)):
    img_input = Input(input_shape)
    x = Conv1D(64 * k, 3, padding='same')(img_input)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Conv1D(64 * k, 3, padding='same')(x)
    x = BatchNormalization()(x)
    block_1_out = Activation('relu')(x)
    x = MaxPooling1D()(block_1_out)
    x = Conv1D(128 * k, 3, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    block_2_out = x
    x = MaxPooling1D()(block_2_out)
    x = Conv1D(256 * k, 3, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    block_3_out = x
    x = MaxPooling1D()(block_3_out)
    x = Conv1D(512 * k, 3, padding='same')(x)
    x = BatchNormalization()(x)
    block_4_out = Activation('relu')(x)
    x = Conv1DTranspose(256 * k, kernel_size=3, strides=2, padding='same')(block_4_out)
    x = concatenate([x, block_3_out])
    x = Conv1DTranspose(128 * k, kernel_size=3, strides=2, padding='same')(x)
    x = concatenate([x, block_2_out])
    x = Conv1DTranspose(64 * k, kernel_size=3, strides=2, padding='same')(x)
    x = concatenate([x, block_1_out])
    x = Conv1D(num_classes, 3, activation='sigmoid', padding='same')(x)
    model = Model(img_input, x)
    model.compile(optimizer=Adam(0.0025), loss='categorical_crossentropy', metrics=[dice_coef])
    return model

def pad_zeros(phrase, xLen=256):
    while len(phrase) < xLen:
        phrase.append([0] * embeddingSize)
    if len(phrase) > xLen:
        phrase = phrase[:xLen]
    return phrase

def getSets(model, senI, tagI):
    xVector = []
    for text in senI:
        tmp = []
        for word in text:
            try:
                tmp.append(model[word])
            except:
                pass
        xVector.append(pad_zeros(tmp, xLen))
    return np.array(xVector, dtype=np.float32), np.array(tagI)

modelGENSIM = word2vec.Word2Vec(xTrain + xTest, size=embeddingSize, window=10, min_count=1, workers=10, iter=10, max_vocab_size=1e5)
GENSIMtrainX, GENSIMtrainY = getSets(modelGENSIM, xTrain, yTrain)
GENSIMtestX, GENSIMtestY = getSets(modelGENSIM, xTest, yTest)

model_b_UNET = create_unet(input_shape=(xLen, embeddingSize))
model_b_UNET.fit(GENSIMtrainX, GENSIMtrainY, validation_data=(GENSIMtestX, GENSIMtestY), epochs=30, batch_size=64)

model_conv1d = create_Conv1d(xLen, embeddingSize)
model_conv1d.fit(GENSIMtrainX, GENSIMtrainY, epochs=50, batch_size=200, validation_data=(GENSIMtestX, GENSIMtestY))

model_b_PSPnet = create_PSPNet(input_shape=(xLen, embeddingSize))
model_b_PSPnet.fit(GENSIMtrainX, GENSIMtrainY, validation_data=(GENSIMtestX, GENSIMtestY), epochs=50, batch_size=512)

def recognizeSet(XX, YY, model, tags, length, value):
    correct_list = np.array([0] * 6)
    incorrect_list = np.array([0] * 6)
    pred = model.predict(XX)
    pred[pred < value] = 0
    pred[pred > value] = 1
    for element in range(YY.shape[0]):
        for word in range(YY.shape[1]):
            for category in range(YY.shape[2]):
                if pred[element][word][category] == YY[element][word][category]:
                    correct_list[category] += 1
                else:
                    incorrect_list[category] += 1
    for i in range(6):
        print(f"Сеть распознала категорию '{tags[i]}' с точностью {round(100 * correct_list[i] / (correct_list[i] + incorrect_list[i]), 2)}%")
    total = round(100 * np.mean(correct_list / (correct_list + incorrect_list)), 2)
    print(f"Средняя точность {total}%")

tags = ['S1', 'S2', 'S3', 'S4', 'S5', 'S6']
recognizeSet(GENSIMtestX, GENSIMtestY, model_conv1d, tags, xLen, 0.97)
